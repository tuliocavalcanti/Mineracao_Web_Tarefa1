FULL PAPER
Is human face processing a feature- or pattern-based task? Evidence
using a unified computational method driven by eye movements
C. E. Thomaza, V. Amarala, G. A. Giraldib, D. F. Gilliesc and D. Rueckertc
aCentro Universitario FEI, Department of Electrical Engineering, Sao Paulo, Brazil
bLaboratorio Nacional de Computacao Cientifica, Rio de Janeiro, Brazil
cImperial College London, Department of Computing, London, UK
ARTICLE HISTORY
Compiled September 6, 2017
ABSTRACT
Research on human face processing using eye movements has provided evidence that
we recognize face images successfully focusing our visual attention on a few inner
facial regions, mainly on the eyes, nose and mouth. To understand how we accom-
plish this process of coding high-dimensional faces so efficiently, this paper proposes
and implements a multivariate extraction method that combines face images vari-
ance with human spatial attention maps modeled as feature- and pattern-based
information sources. It is based on a unified multidimensional representation of the
well-known face-space concept. The spatial attention maps are summary statistics
of the eye-tracking fixations of a number of participants and trials to frontal and
well-framed face images during separate gender and facial expression recognition
tasks. Our experimental results carried out on publicly available face databases
have indicated that we might emulate the human extraction system as a pattern-
based computational method rather than a feature-based one to properly explain
the proficiency of the human system in recognizing visual face information.
KEYWORDS
Human face processing; eye movements; task-driven face dimensions
1. Introduction
Research on human face processing using eye movements has consistently demon-
strated the existence of preferred facial regions or pivotal areas involved in successful
identity, gender and facial expression human recognition [1, 2, 3, 4, 5]. These works
have provided evidence that humans analyze faces focusing their visual attention on a
few inner facial regions, mainly on the eyes, nose and mouth, and such sparse spatial
fixations are not equally distributed and depend on the face perception task under in-
vestigation. Faces, therefore, are not only well-known visual stimuli that can be easily
recognized by humans [6], but also serve as examples of the human ability of extracting
relevant visual information very efficiently from the corresponding stimuli to recognize
objects of interest with high proficiency.
However, the computational representation and modeling of this apparently natural
and heritable embedded human ability [7, 8] remains challenging. Although faces are
expected to have a global and common spatial layout with all their parts such as eyes,
CONTACT C. E. Thomaz. Email: cet@fei.edu.br
ar
X
iv
:1
70
9.
01
18
2v
1 
 [
cs
.C
V
] 
 4
 S
ep
 2
01
7
nose and mouth arranged consistently in a multidimensional representation, specific
variations in local parts are fundamental to explain our perception of each individual
singularity, or groups of individuals when distinguishing, for example, between gen-
der or facial expression [9, 10, 11, 12, 13, 14, 15]. To understand and emulate how
humans accomplish this process of coding faces, it seems necessary to investigate and
develop computational extraction methods that explore the combination and relative
interaction of the global and local types of information, considering as well the em-
bedded visual knowledge that might be behind the human face perception task under
investigation.
In this work, we describe and implement a unified computational method that com-
bines global and local variance information with eye-tracking fixations to represent
task-driven dimensions in a multidimensional physiognomic face-space. More impor-
tantly, this unified computational method allows the exploration of two distinct em-
bedded knowledge extractions, named here feature- and pattern-based information
processing, to disclose some evidence of how humans perceive faces visually. The eye-
tracking fixations are based on measuring eye movements of a number of participants
and trials to frontal and well-framed face images during separate gender and facial
expression classification tasks. In all the automatic classification experiments carried
out to evaluate the unified computational method proposed, we have considered: dif-
ferent numbers of face-space dimensions; gender and facial expression sparse spatial
fixations; and randomly generated versions of the distribution of the human eye fixa-
tions spread across faces. These randomly generated spatial attention maps pose the
alternative analysis where there are no preferred viewing positions for human face
processing, contrasting the literature findings.
The paper is organized as follows. In section 2, we describe the eye-tracking appa-
ratus, participants, and frontal face stimuli used to generate different fixation images
depending on the classification task. Then, section 3 translates in a unified method the
combination of face-space dimensions and eye movements sources of information for
feature- and pattern-based multivariate computational analysis. Section 4 describes
the eye-tracking experiments carried out and the training and test face samples used
from distinct image datasets to evaluate the automatic classification accuracy of the
method. All the results have been analyzed in section 5. Finally, in section 6, we
conclude the paper, discussing its main contribution and limitation.
2. Materials
In this section, we describe mainly the eye-tracking apparatus, participants and frontal
face stimuli used to generate different fixation images with distinct classification tasks.
2.1. Apparatus
Eye movements were recorded with an on-screen Tobii TX300 equipment that com-
prises an eye tracker unit integrated to the lower part of a 23in TFT monitor. The
eye tracker performs binocular tracking at a data sampling rate of 300Hz, and has
minimum fixation duration of 60ms and maximum dispersion threshold of 0.5 degrees.
These are the eye tracker defaults for cognitive research. A standard keyboard was
used to collect participants responses. Calibration, monitoring and data collection
were performed as implemented in the Tobii Studio software running on an attached
notebook (Core i7, 16Gb RAM and Windows 7).
2
2.2. Participants
A total number of 44 adults (26 males and 18 females) aged from 18 to 50 years
participated in this study on a voluntary basis. All participants were undergraduate
and graduate students or staff at the university and had normal or corrected to normal
vision. Written informed consent was obtained from all participants.
2.3. Training face database
Frontal images of the FEI face database [16] have been used to carry out the eye-
movements stimuli. This database contains 400 frontal 2D face images of 200 subjects
(100 men and 100 women). Each subject has two frontal images, one with a neutral
or non-smiling expression and the other with a smiling facial expression. We have
used a rigidly registered version of this database where all these frontal face images
were previously aligned using the positions of the eyes as a measure of reference.
The registered and cropped images are 128 pixels wide and 128 pixels high and are
encoded in gray-scale using 8-bits per pixel. Figure 1 illustrates some of these well-
framed images.
Figure 1. A sample of the face stimuli used in this work.
2.4. Stimuli
Stimuli consisted of 120 frontal face images taken from the training face database. All
the stimuli were presented centralized on a black background using the 23in TFT mon-
itor with a screen resolution of 1280x1024 pixels. To improve the stimuli visualization
on the TFT monitor all the face images were resized to 512x512 pixels. Presentation
of the stimuli was controlled by the Tobii Studio software.
2.5. Spatial attention maps
Eye-movements were processed directly from the eye tracker using the Tobii Studio
software. Fixation was defined by the standard Tobii fixation filter as two or more
consecutive samples falling within a 50-pixel radius. We considered only data from
participants for whom on average 25% or more of their gaze samples were collected
by the eye tracker. One participant (1 female) did not meet this criterion and was
excluded from the analysis. The standard absolute duration heat maps available at
the Tobii Studio software were used to describe the accumulated fixation duration on
different locations in the face images at the resolution of 512x512 pixels. These absolute
duration heat maps were averaged from all participants and from all face stimuli,
generating different fixation images, called here spatial attention maps, depending on
the classification task.
3
3. Method
The computational method combines face images variance with the spatial attention
maps modeled as feature- and pattern-based information sources in a multidimen-
sional representation of the face-space physiognomic dimensions [17, 18]. It builds on
our previous works [16, 19, 4] of incorporating task-driven information for a unified
multivariate computational analysis of face images using human visual processing.
3.1. Face-space dimensions
We have used principal components to specify the face-space physiognomic dimensions
[17, 18] because of their psychological plausibility for understanding the human face
image multidimensional representation [20, 21, 22, 23, 18], where the whole face is
perceived as a single entity [24].
A single entity face image, with c pixels wide and r pixels high, can be described
mathematically as a single point in an n-dimensional space by concatenating the rows
(or columns) of its image matrix [20], where n = c× r. The coordinates of this point
describe the values of each pixel of the image and form a n-dimensional 1D vector
x = [x1, x2, . . . , xn]
T .
Let an N × n data matrix X be composed of N face images with n pixels, that is,
X = (x1,x2, . . . ,xN )
T . This means that each column of matrix X represents the values
of a particular pixel all over the N images. Let this data matrix X have covariance
matrix
S =
1
(N ? 1)
N?
i=1
(xi ? x?)(xi ? x?)T , (1)
where x? is the grand mean vector of X given by
x? =
1
N
N?
i=1
xi = [x?1, x?2, . . . , x?n]
T . (2)
Let this covariance matrix S have respectively P and ? eigenvector and eigenvalue
matrices, that is,
P TSP = ?. (3)
It is a proven result that the set of m (m ? n) eigenvectors of S, which corre-
sponds to the m largest eigenvalues, minimizes the mean square reconstruction er-
ror over all choices of m orthonormal basis vectors [25]. Such a set of eigenvectors
P = [p1,p2, . . . ,pm] that defines a new uncorrelated coordinate system for the data
matrix X is known as the (standard) principal components.
The calculation of the standard principal components is based entirely on the data
matrix X and does not express any domain specific information about the face per-
ception task under investigation. We describe next modifications on this calculation
that handle global and local facial differences using feature- and pattern-based com-
binations of variance and the spatial attention maps.
4
3.2. Feature-based combination of variance and spatial attention map
(wPCA)
We can rewrite the sample covariance matrix S described in equation (1) in order to
indicate the spatial association between the n pixels in the N samples as separated n
features. When n pixels are observed on each face image, the sample variation can be
described by the following sample variance-covariance equation [26]:
S = {sjk} =
{
1
(N ? 1)
N?
i=1
(xij ? x?j)(xik ? x?k)
}
, (4)
for j = 1, 2, . . . , n and k = 1, 2, . . . , n. The covariance sjk between the j
th and kth
pixels reduces to the sample variance when j = k, sjk = skj for all j and k, and the
covariance matrix S contains n variances and 12n(n?1) potentially different covariances
[26]. It is clear from equation (4) that each pixel deviation from its mean has the same
importance in the standard sample covariance matrix S formulation.
To combine these pixel-by-pixel deviations with the visual information captured by
the eye movements, we first represent, analogously to the face images, the correspond-
ing spatial attention map as a n-dimensional 1D w vector, that is,
w = [w1, w2, . . . , wn]
T , (5)
where wj ? 0 and
?n
j=1wj = 1. Each wj describes the visual attention power of
the jth pixel separately. Thus, when n pixels are observed on N samples, the sample
covariance matrix S? can be described by [19, 4]
S? =
{
s?jk
}
=
{
1
(N ? 1)
N?
i=1
?
wj(xij ? x?j)
?
wk(xik ? x?k)
}
. (6)
It is important to note that s?jk = s
?
kj for all j and k and consequently the matrix
S? is a nxn symmetric matrix. Let S? have respectively P ? and ?? eigenvector and
eigenvalue matrices, as follows:
P ?TS?P ? = ??. (7)
The set of m? (m? ? n) eigenvectors of S?, that is, P ? = [p?1,p?2, . . . ,p?m? ], which
corresponds to the m? largest eigenvalues, defines the orthonormal coordinate system
for the data matrixX called here feature-based principal components or, simply, wPCA.
The step-by-step algorithm for calculating these feature-based principal components
can be summarized as follows:
(1) Calculate the spatial attention map w = [w1, w2, . . . , wn]
T by averaging the
fixation locations and durations from face onset from all participants and from
all face stimuli for the classification task considered;
(2) Normalize w, such that wj ? 0 and
?n
j=1wj = 1, by replacing wj with
|wj |?n
j=1 |wj |
;
(3) Standardize all the n variables of the data matrix X such that the new variables
have x?j = 0, for j = 1, 2, . . . , n. In other words, calculate the grand mean vector
as described in Equation (2) and replace xij with zij , where zij = xij ? x?j for
i = 1, 2, . . . , N and j = 1, 2, . . . , n;
5
(4) Spatially weigh up all the standardized zij variables using the vector w calculated
in step 2, that is, z?ij = zij
?
wj ;
(5) The feature-based principal components P ? are then the eigenvectors corre-
sponding to the m? largest eigenvalues of Z?(Z?)T , where Z? = {z?1, z?2, . . . , z?N}
T
and m? ? n.
3.3. Pattern-based combination of variance and spatial attention map
(dPCA)
We can handle the problem of combining face samples variance with the perceptual
processing captured by the eye movements assuming a pattern-based approach rather
than a feature-based one as previously described. Here, we would like to investigate the
spatial association between the features with their perceptual interaction preserved,
not treated separately.
The set of n-dimensional eigenvectors P = [p1,p2, . . . ,pm] is defined in equation
(3) as the standard principal components, and the n-dimensional w spatial attention
representation, where w = [w1, w2, . . . , wn]
T , is described in equation (5).
To determine the perceptual contribution of each standard principal component we
can calculate how well these face-space directions align with the corresponding spatial
attention map, that is, how well p1,p2, . . . ,pm align with w, as follows:
k1 = w
T · p1, (8)
k2 = w
T · p2,
...
km = w
T · pm.
Coefficients ki, where i = 1, 2, . . . ,m, that are estimated to be 0 or approximately 0
have negligible contribution, indicating that the corresponding principal component
directions are not relevant. In contrast, largest coefficients (in absolute values) indi-
cate that the corresponding variance directions contribute more and consequently are
important to characterize the human perceptual processing.
We select then as the first principal components [16] the ones with the highest visual
attention coefficients, that is,
P+ = [p+1 ,p
+
2 , ...,p
+
m+ ] = arg max
P
??P TSP ??, (9)
where {p+i |i = 1, 2, . . . m+} is the set of eigenvectors of S corresponding to the largest
coefficients |k1| ? |k2| ? . . . ? |km| calculated in Equation (8), where (m+ < m ? n).
The set of m+ eigenvectors of S, that is, P+ = [p+1 ,p
+
2 , . . . ,p
+
m+ ], defines the or-
thonormal coordinate system for the data matrix X called here pattern-based principal
components or, simply, dPCA.
The step-by-step algorithm for calculating these pattern-based principal compo-
nents can be summarized as follows:
(1) Calculate the spatial attention map w = [w1, w2, . . . , wn]
T by averaging the
fixation locations and durations from face onset from all participants and from
all face stimuli for the classification task considered;
(2) Normalize w, such that wj ? 0 and
?n
j=1wj = 1, by replacing wj with
|wj |?n
j=1 |wj |
;
6
(3) Calculate the covariance matrix S of the data matrix X and then its respec-
tively P and ? eigenvector and eigenvalue matrices. Retain all the m non-zero
eigenvectors P = [p1,p2, . . . ,pm] of S, where ?(j) > 0 for j = 1, 2, . . . ,m and
m ? n;
(4) Calculate the spatial attention coefficient of each non-zero eigenvector using the
vector w described in step 2, as follows: kj = w
T · pj , for j = 1, 2, . . . ,m;
(5) The pattern-based principal components P+ = [p+1 ,p
+
2 , . . . ,p
+
m+ ], where m
+ <
m, are then the eigenvectors of S corresponding to the largest coefficients |k1| ?
|k2| ? . . . ? |km+ | ? . . . ? |km|.
3.4. Geometric Idea
We show in Figure 2 the main geometric idea of the feature- and pattern-based princi-
pal components. The hypothetical illustration presents samples depicted by triangles
along with the spatial attention vector w represented in red.
It is well known that the standard principal components [p1,p2] are obtained by
rotating the original coordinate axes until they coincide with the axes of the constant
density ellipse described by all the samples.
On the one hand, the feature-based approach uses the information provided by w
for each original variable isolated to finding a new orthonormal basis that is not neces-
sarily composed of the same principal components. In other words, in this hypothetical
example the influence of the variable deviations on the x2 axis will be relatively mag-
nified in comparison with x1 because w is better aligned to the original x2 axis than
x1 one, that is, |w2| > |w1|. This is geometrically represented in the figure by large
gray arrows, which indicate visually that the constant density ellipse will be expanded
in the x2 axis and shrunk in the x1 axis, changing the original spread of the sam-
ples illustrated in black to possibly the constant density ellipse represented in blue.
Therefore, the feature-based principal components [p?1,p
?
2] would be different from the
standard principal components [p1,p2], because p
?
1 is expected to be much closer to
the x2 direction than x1, providing a new interpretation of the original data space
based on the power of each variable considered separately.
On the other hand, the pattern-based approach uses the information provided by
w as a full two-dimensional pattern, ranking the standard principal components by
how well they align with the entire pattern captured by w across the two-dimensional
space samples. Since w as a whole is better aligned to the second standard principal
component p2 direction than the first one p1, i.e. |wT ·p2| > |wT ·p1|, then p+1 = p2 and
p+2 = p1. Therefore, the pattern-based approach selects as its first principal component
the standard variance direction that is most efficient for describing the whole spatial
attention map, which comprises here the entire pattern of the eye movements across
the whole faces, rather than representing all the pixels visual attention power as unit
apart features.
4. Experiments
The experiments consisted of two separate and distinct classification tasks: (1) gender
(male versus female) and (2) facial expression (smiling versus neutral). During the
gender experiments, 60 faces equally distributed among gender were shown on the
Tobii eye tracker, all with neutral facial expression (30 males and 30 females). For the
facial expression experiments, all the 60 faces shown were equally distributed among
7
Figure 2. An hypothetical example that shows samples (depicted by two-dimensional points represented by
triangles) and the geometric idea of the feature- and pattern-based approaches. The former magnifies or shrinks
the deviation of each variable separately depending on the direction of w, where |w2| > |w1|, and [p1,p2] and
[p?1,p
?
2] are respectively the standard and feature-based principal components. The latter re-ranks the standard
principal components [p1,p2] by how well such directions align with w as a whole, where |wT ·p2| > |wT ·p1|
and [p+1 ,p
+
2 ] are consequently the pattern-based principal components.
gender and facial expression (15 males smiling, 15 females smiling, 15 males with
neutral expression and 15 females with neutral expression).
Participants were seated in front of the eye tracker at a distance of 60cm and
initially filled out an on-screen questionnaire about their gender, ethnicity, age and
motor predominance. They were then instructed to classify the faces using their index
fingers to press the corresponding two keys on the keyboard. Participants were asked
to respond as accurately as possible and informed that there was no time limit for
their responses. Each task began with a calibration procedure as implemented in the
Tobii Studio software. On each trial, a central fixation cross was presented for 1 second
followed by a face randomly selected for the corresponding gender or facial expression
experimental samples. The face stimulus was presented for 3 seconds in both tasks and
was followed by a question on a new screen that required a response in relation to the
8
experiment, that is, ”Is it a face of a (m)ale or (f)emale subject?” or ”Is it a face of
a (s)miling or (n)eutral facial expression subject?”. Each response was subsequently
followed by the central fixation cross, which preceded the next face stimulus until all
the 60 faces were presented for each classification task. Each participant completed 60
trials for the gender classification task and 60 trials for the facial expression one with
a short break in between the tasks.
We adopted a 10-fold cross validation method drawn at random from the gender and
smiling corresponding sample groups to evaluate the automatic classification accuracy
of the feature- and pattern-based dimensions. Additionally to the FEI face samples
described previously and used for training only, we have used frontal face images of
the well-known FERET database [27], registered analogously to the FEI samples, for
testing. In the FERET database, we have also considered 200 subjects (107 men and
93 women) and each subject has two frontal images (one with a neutral or non-smiling
expression and the other with a smiling facial expression), providing a total of two
distinct training and test sets of 400 images to perform the gender and expression
automatic classification experiments. We have assumed that the prior probabilities
and misclassification costs are equal for both groups. On the principal components
subspace, the standard sample group mean of each class has been calculated from
the corresponding training images and the minimum Mahalanobis distance from each
class mean [25] has been used to assign a test observation to either the smiling and
non-smiling classes in the facial expression experiment, or to either the male or female
classes in the gender experiment.
In all the automatic classification experiments, we have considered different num-
bers of principal components, both gender and facial expression task-driven spatial
attention maps accordingly, and their corresponding randomly generated versions with
the distribution of the human eye fixations uniformly spread across faces to pose the
alternative analysis where there are no preferred viewing positions for human face
processing.
5. Results
The classification results of the 43 participants on the gender and facial expression
tasks were all above the chance level (50%). Their performance on the male versus fe-
male (gender) and smiling versus non-smiling (facial expression) tasks were on average
97.2%(±2.3%) and 92.8%(±4.3%), respectively.
Figure 3 illustrates the spatial attention maps (left side) along with their corre-
sponding randomly generated versions (right side) used to calculate the feature- and
pattern-based principal components. The spatial attention maps (left) are summary
statistics that describe the central tendency of the fixation locations and durations
from face onset from all participants and from all face stimuli after 3 seconds for
the facial expression (top panel) and gender (bottom) classification tasks. We have
disregarded the first two fixations of all participants to avoid the central cross bias.
There are location similarities in the manner all the faces were perceived, highlighting
relevant proportion of fixations directed at mainly the pivotal areas of both eyes, nose
and mouth. These results show that participants made slightly different fixations in
the two classification tasks on these areas known as optimal for human processing of
the entire faces [1, 2, 5]. In contrast, as expected, the randomly generated versions
(right) of the spatial attention maps are uniformly distributed, showing essentially a
sub-sampling of the entire face without any preferred features or viewing positions.
9
Figure 3. An illustration of the spatial attention maps (left) and their corresponding randomly generated
versions (right). The upper and lower panels describe the facial expression and gender classification tasks,
respectively, superimposed on the grand mean face of the training database used.
Figure 4 shows the recognition rates of the facial expression (top panel) and gender
experiments (bottom panel) for the feature- and pattern-based principal components
using the spatial attention maps and their corresponding random versions. The num-
ber of principal components considered varied from 20 to 240 because all the recogni-
tion rates leveled off or decreased with additional components. We can see that both
feature- and pattern-based automatic mappings of the high-dimensional face images
into lower-dimensional spaces are accurately equivalent, with no significant statisti-
cal difference on their recognition rates, when using the facial expression and gender
spatial attention maps to highlight accordingly the preferred inner face regions for
automatic classification. In other words, both processings have shown to be compu-
tationally effective to automatically classify the facial expression and gender samples
used.
However, the feature-based behavior is noteworthy when using the random versions
of the aforementioned maps. In Figure 4, there is no statistical difference between
10
the feature-based and its random version of results on the facial expression experi-
ments and, in fact, there is some statistical significant improvement (p < 0.05) on
its automatic recognition performance when using the randomly generated version of
the spatial attention maps in the gender experiments. On the condition of analyzing
frontal and well-framed face images, the results indicate that the feature-based ap-
proach shows no specific exploitation of the manner in which all the participants have
viewed the entire faces when classifying the samples. These results suggest indeed that
there is no critical region or pivotal areas involved in successful gender and facial ex-
pression human recognition, despite the clear evidence of focused visual attention on
the eyes, nose and mouth described in the previous Figure 3.
Interestingly, though, the findings of the pattern-based dimensions are exactly the
opposite. We can see clearly in Figure 4 statistical differences between the pattern-
based and its random version results (p < 0.001) on both facial expression and gender
experiments, where the preferred participants fixation positions augment considerably
the automatic recognition of their face-space dimensions when using the spatial atten-
tion maps rather than their randomly generated versions. These results provide mul-
tivariate statistical evidence that faces seem to be analysed visually using a pattern-
based strategy, instead of decomposing such information processing into separate and
discrete local features.
6. Conclusion
This work provides theoretical and empirical evidence about the processes underlying
human face visual cognition using frontal and well-framed face images as stimuli.
Exploring eye movements of a number of participants on gender and facial expression
distinct classification tasks, we have been able to implement an automatic multivariate
statistical extraction method that combines variance information with sparse visual
processing about the task-driven experiment.
Our experimental results carried out on publicly available face databases have sug-
gested that the proficiency of the human face processing can be explained computa-
tionally as a pattern-based feature extraction process, but not as a feature-by-feature
one. The pattern-based computation of the spatial association between the face image
features with their visual perceptual interaction preserved has shown statistically sig-
nificant differences between the preferred human eye fixations and their randomly gen-
erated versions, confirming systematically the literature findings of few but sufficient
viewing positions for successful human face recognition. Conversely, the feature-based
computational results suggest that there is no critical region or pivotal areas involved
in such processing, and a random sub-sampling of the entire face without any preferred
features or viewing positions provides comparable accuracies, despite the well-known
evidence, shown in this work as well, for focused human visual attention on the eyes,
nose and mouth. In short, to convey the visual cues that might create a perceptu-
ally plausible approach for coding face images for recognition, the results described
here indicate that we might emulate the human extraction system as a pattern-based
computational method rather than a feature-based one to advance the development
of more efficient methods for the spatial analysis of visual face information.
All the analyses presented here are based on the concept of psychologically plausible
face-space dimensions described as principal components of high-dimensional image
vectors. Although this concept has become established as an effective linear model
for representing the dimensions of variation that occur in collections of human faces
11
Figure 4. Facial expression (top) and gender (bottom) boxplots of the recognition rates of feature-based
(wPCA) and pattern-based (dPCA) dimensions given the corresponding spatial attention maps and their
randomly generated versions (rnd). The number of principal components considered for automatic classification
varied from 20 to 240.
[17, 20, 21, 24, 22, 23, 18], it needs to reshape the 2D face input images into 1D
vectors, breaking the natural structure of the human face perception. Further work
12
on multilinear subspace learning might result in more compact representation of the
human face processing, preserving the original data structure and leading perhaps to
a more effective computational approach for coding face images when reducing the
dimensionality of the high-dimensional image inputs.
Acknowledgement(s)
The authors would like to thank the financial support provided by FAPESP
(2012/22377-6) and CNPq (309532/2014-0). This work is part of the Newton Ad-
vanced Fellowship appointed by the Royal Society to Carlos E. Thomaz.
References
[1] Hsiao JHW, Cottrell G. Two fixations suffice in face recognition. Psychological
Science. 2008;19(10):998–1006.
[2] Peterson MF, Eckstein MP. Looking just below the eyes is optimal across
face recognition tasks. Proceedings of the National Academy of Sciences. 2012;
109(48):E3314–E3323.
[3] Moreno EP, Ferreiro VR, Gutierrez AG. Where to look when looking at faces: Vi-
sual scanning is determined by gender, expression and task demands. Psicologica.
2016;37:127–150.
[4] Thomaz CE, Amaral V, Gillies DF, et al. Priori-driven dimensions of face-space:
Experiments incorporating eye-tracking information. In: Proceedings of the Ninth
Biennial ACM Symposium on Eye Tracking Research & Applications; 2016. p.
279–282; ETRA ’16.
[5] Bobak AK, Parris BA, Gregory NJ, et al. Eye-movement strategies in develop-
mental prosopagnosia and ”super” face recognition. The Quarterly Journal of
Experimental Psychology. 2017;70(2):201–217.
[6] Crookes K, McKone E. Early mature of face recognition: no childhood develop-
ment of holistic processing, novel face encoding, or face-space. Cognition. 2009;
111:219–247.
[7] Wilmer JB, Germine L, Chabris CF, et al. Human face recognition ability is
specific and highly heritable. Proceedings of the National Academy of Sciences of
the United States of America. 2010;107(11):5238–5241.
[8] McKone E, Palermo R. A strong role for nature in face recognition. Proceed-
ings of the National Academy of Sciences of the United States of America. 2010;
107(11):4795–4796.
[9] Cabeza R, Kato T. Features are also important: Contributions of featural and
configural processing to face recognition. Psychological Science. 2000;11(5):429–
433.
[10] Joseph RM, Tanaka J. Holistic and part-based face recognition in children with
autism. Journal of Child Psychology and Psychiatry. 2003;44(4):529–542.
[11] Wallraven C, Schwaninger A, Bulthoff HH. Learning from humans: Computa-
tional modeling of face recognition. Network: Computation in Neural Systems.
2005;16(4):401–418.
[12] Maurer D, OCraven KM, Grand R, et al. Neural correlates of processing fa-
cial identity based on features versus their spacing. Neuropsychologia. 2007;
45(7):1438–1451.
13
[13] Shin NY, Jang JH, Kwon JS. Face recognition in human: the roles of featural and
configurational processing. Face Analysis, Modelling and Recognition Systems.
2011;9:133–148.
[14] Miellet S, Caldara R, Schyns PG. Local jekyll and global hyde: The dual identity
of face recognition. Psychological Science. 2011;22(12):1518–1526.
[15] Seo J, Park H. Robust recognition of face with partial variations using local
features and statistical learning. Neurocomputing. 2014;129:41–48.
[16] Thomaz CE, Giraldi GA. A new ranking method for principal components analy-
sis and its application to face image analysis. Image and Vision Computing. 2010;
28(6):902–913.
[17] Valentine T. A unified account of the effects of distinctiveness, inversion, and race
in face recognition. The Quarterly Journal of Experimental Psychology Secion A:
Human Experimental Psychology. 1991;43:161–204.
[18] Valentine T, Lewis MB, Hills PJ. Face-space: A unifying concept in face recogni-
tion research. The Quarterly Journal of Experimental Psychology. 2015;:1–24.
[19] Thomaz CE, Giraldi GA, Costa JP, et al. A priori-driven PCA. In: ACCV 2012
Workshops (LNCS 7729); Springer; 2013. p. 236–247.
[20] Sirovich L, Kirby M. Low-dimensional procedure for the characterization of hu-
man faces. Journal of Optical Society of America. 1987;4:519–524.
[21] Hancock PJB, Burton MA, Bruce V. Face processing: Human perception and
principal component analysis. Memory and Cognition. 1996;24(1):26–40.
[22] Todorov A, Oosterhof NN. Modeling social perception of faces. IEEE Signal Pro-
cessing Magazine. 2011;March:117–122.
[23] Frowd CD. Facial composite systems. Forensic facial identification: Theory and
practice of identification from eyewitnesses, composites and CCTV. 2015;:43–70.
[24] Rakover SS. Featural vs. configurational information in faces: A conceptual and
empirical analysis. British Journal of Psychology. 2002;93:1–30.
[25] Fukunaga K. Introduction to statistical pattern recognition. Academic Press, New
York; 1990.
[26] Johnson R, Wichern D. Applied multivariate statistical analysis. New Jersey:
Prentice Hall; 1998.
[27] Philips PJ, Wechsler H, Huang J, et al. The feret database and evaluation
procedure for face recognition algorithms. Image and Vision Computing. 1998;
16(5):295–306.
14
